{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment in Google Colab\n",
    "Configure Google Colab environment, check for GPU availability, and mount Google Drive for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU')\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change directory to the project folder in Google Drive\n",
    "import os\n",
    "project_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification'\n",
    "if not os.path.exists(project_path):\n",
    "    os.makedirs(project_path)\n",
    "os.chdir(project_path)\n",
    "print(f'Changed working directory to {project_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install YOLOv8\n",
    "Install the Ultralytics YOLOv8 package and other required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install YOLOv8\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install other required dependencies\n",
    "!pip install opencv-python-headless\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "\n",
    "# Verify the installation\n",
    "import ultralytics\n",
    "print(f'Ultralytics YOLOv8 version: {ultralytics.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Clothing Dataset\n",
    "Download and prepare a clothing dataset (like DeepFashion or Fashion-MNIST), organize images into appropriate folders for classification and segmentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare a clothing dataset (like DeepFashion or Fashion-MNIST), organize images into appropriate folders for classification and segmentation tasks.\n",
    "\n",
    "# Download the DeepFashion dataset\n",
    "!wget http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval/img.zip -O deepfashion.zip\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip deepfashion.zip -d deepfashion\n",
    "\n",
    "# Organize images into appropriate folders for classification and segmentation tasks\n",
    "import shutil\n",
    "\n",
    "# Create directories for classification and segmentation\n",
    "classification_dir = os.path.join(project_path, 'classification')\n",
    "segmentation_dir = os.path.join(project_path, 'segmentation')\n",
    "\n",
    "if not os.path.exists(classification_dir):\n",
    "    os.makedirs(classification_dir)\n",
    "if not os.path.exists(segmentation_dir):\n",
    "    os.makedirs(segmentation_dir)\n",
    "\n",
    "# Move images to classification directory\n",
    "for root, dirs, files in os.walk('deepfashion'):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            shutil.move(os.path.join(root, file), classification_dir)\n",
    "\n",
    "# Note: For segmentation tasks, you would need to have corresponding mask images.\n",
    "# This example assumes that the dataset contains only images for classification.\n",
    "# If you have mask images, you can move them to the segmentation directory similarly.\n",
    "\n",
    "print(f'Images moved to {classification_dir} for classification tasks.')\n",
    "print(f'Prepare your segmentation masks and move them to {segmentation_dir} for segmentation tasks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure YOLOv8 for Classification and Segmentation\n",
    "Set up YOLOv8 configurations for both classification and segmentation tasks, create YAML configuration files for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure YOLOv8 for Classification and Segmentation\n",
    "\n",
    "# Create YAML configuration files for the dataset\n",
    "classification_yaml = \"\"\"\n",
    "train: {classification_dir}/train\n",
    "val: {classification_dir}/val\n",
    "nc: 10  # Number of classes\n",
    "names: ['T-shirt', 'Shirt', 'Dress', 'Jeans', 'Shorts', 'Skirt', 'Jacket', 'Sweater', 'Shoes', 'Bag']\n",
    "\"\"\"\n",
    "\n",
    "segmentation_yaml = \"\"\"\n",
    "train: {segmentation_dir}/train\n",
    "val: {segmentation_dir}/val\n",
    "nc: 10  # Number of classes\n",
    "names: ['T-shirt', 'Shirt', 'Dress', 'Jeans', 'Shorts', 'Skirt', 'Jacket', 'Sweater', 'Shoes', 'Bag']\n",
    "\"\"\"\n",
    "\n",
    "# Save the YAML configuration files\n",
    "with open(os.path.join(project_path, 'classification.yaml'), 'w') as file:\n",
    "    file.write(classification_yaml)\n",
    "\n",
    "with open(os.path.join(project_path, 'segmentation.yaml'), 'w') as file:\n",
    "    file.write(segmentation_yaml)\n",
    "\n",
    "print('YAML configuration files created for classification and segmentation tasks.')\n",
    "\n",
    "# Load YOLOv8 model for classification\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize the model for classification\n",
    "model_classification = YOLO('yolov8n-cls.pt')  # Use a pre-trained YOLOv8 model for classification\n",
    "\n",
    "# Configure the model for training\n",
    "model_classification.train(data=os.path.join(project_path, 'classification.yaml'), epochs=50, imgsz=224)\n",
    "\n",
    "# Load YOLOv8 model for segmentation\n",
    "model_segmentation = YOLO('yolov8n-seg.pt')  # Use a pre-trained YOLOv8 model for segmentation\n",
    "\n",
    "# Configure the model for training\n",
    "model_segmentation.train(data=os.path.join(project_path, 'segmentation.yaml'), epochs=50, imgsz=640)\n",
    "\n",
    "print('YOLOv8 models configured for classification and segmentation tasks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Classification Model\n",
    "Train YOLOv8 classification model on the clothing dataset with appropriate hyperparameters and augmentation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classification Model\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU')\n",
    "\n",
    "# Initialize the YOLOv8 model for classification\n",
    "model_classification = YOLO('yolov8n-cls.pt')  # Use a pre-trained YOLOv8 model for classification\n",
    "\n",
    "# Configure the model for training\n",
    "model_classification.train(data=os.path.join(project_path, 'classification.yaml'), epochs=50, imgsz=224, device=device)\n",
    "\n",
    "print('YOLOv8 classification model training completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Segmentation Model\n",
    "Train YOLOv8 segmentation model to identify clothing items and create masks for background removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Segmentation Model\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU')\n",
    "\n",
    "# Initialize the YOLOv8 model for segmentation\n",
    "model_segmentation = YOLO('yolov8n-seg.pt')  # Use a pre-trained YOLOv8 model for segmentation\n",
    "\n",
    "# Configure the model for training\n",
    "model_segmentation.train(data=os.path.join(project_path, 'segmentation.yaml'), epochs=50, imgsz=640, device=device)\n",
    "\n",
    "print('YOLOv8 segmentation model training completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Evaluate both models using metrics like precision, recall, mAP, and visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate classification model\n",
    "def evaluate_classification_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate segmentation model\n",
    "def evaluate_segmentation_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming data loaders for validation data are available as val_loader_classification and val_loader_segmentation\n",
    "# Evaluate the classification model\n",
    "evaluate_classification_model(model_classification, val_loader_classification)\n",
    "\n",
    "# Evaluate the segmentation model\n",
    "evaluate_segmentation_model(model_segmentation, val_loader_segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Background Removal\n",
    "Implement a pipeline to use the trained segmentation model to remove backgrounds from clothing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Background Removal\n",
    "\n",
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function to remove background using the segmentation model\n",
    "def remove_background(image_path, model):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocess the image for the model\n",
    "    input_image = cv2.resize(image_rgb, (640, 640))\n",
    "    input_image = input_image / 255.0\n",
    "    input_image = np.transpose(input_image, (2, 0, 1))\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    input_image = torch.tensor(input_image, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Get the segmentation mask from the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)[0]\n",
    "    mask = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Resize the mask to the original image size\n",
    "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Create a binary mask\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "    \n",
    "    # Apply the mask to the image\n",
    "    result = cv2.bitwise_and(image_rgb, image_rgb, mask=binary_mask)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "image_path = 'path_to_your_image.jpg'  # Replace with the path to your image\n",
    "result = remove_background(image_path, model_segmentation)\n",
    "\n",
    "# Display the original image and the result\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Background Removed')\n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export and Save the Models\n",
    "Export the trained models in appropriate formats (ONNX, TorchScript, etc.) and save them to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Save the Models\n",
    "\n",
    "# Export the trained classification model to ONNX format\n",
    "classification_onnx_path = os.path.join(project_path, 'yolov8_classification.onnx')\n",
    "model_classification.export(format='onnx', path=classification_onnx_path)\n",
    "print(f'Classification model exported to {classification_onnx_path}')\n",
    "\n",
    "# Export the trained segmentation model to ONNX format\n",
    "segmentation_onnx_path = os.path.join(project_path, 'yolov8_segmentation.onnx')\n",
    "model_segmentation.export(format='onnx', path=segmentation_onnx_path)\n",
    "print(f'Segmentation model exported to {segmentation_onnx_path}')\n",
    "\n",
    "# Save the trained classification model to TorchScript format\n",
    "classification_torchscript_path = os.path.join(project_path, 'yolov8_classification.pt')\n",
    "model_classification.save(path=classification_torchscript_path)\n",
    "print(f'Classification model saved to {classification_torchscript_path}')\n",
    "\n",
    "# Save the trained segmentation model to TorchScript format\n",
    "segmentation_torchscript_path = os.path.join(project_path, 'yolov8_segmentation.pt')\n",
    "model_segmentation.save(path=segmentation_torchscript_path)\n",
    "print(f'Segmentation model saved to {segmentation_torchscript_path}')\n",
    "\n",
    "# Save models to Google Drive\n",
    "drive_classification_onnx_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_classification.onnx'\n",
    "drive_segmentation_onnx_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_segmentation.onnx'\n",
    "drive_classification_torchscript_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_classification.pt'\n",
    "drive_segmentation_torchscript_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_segmentation.pt'\n",
    "\n",
    "shutil.copy(classification_onnx_path, drive_classification_onnx_path)\n",
    "shutil.copy(segmentation_onnx_path, drive_segmentation_onnx_path)\n",
    "shutil.copy(classification_torchscript_path, drive_classification_torchscript_path)\n",
    "shutil.copy(segmentation_torchscript_path, drive_segmentation_torchscript_path)\n",
    "\n",
    "print('Models saved to Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
