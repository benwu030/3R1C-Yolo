{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment in Google Colab\n",
    "Configure Google Colab environment, check for GPU availability, and mount Google Drive for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU')\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change directory to the project folder in Google Drive\n",
    "import os\n",
    "project_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification'\n",
    "if not os.path.exists(project_path):\n",
    "    os.makedirs(project_path)\n",
    "os.chdir(project_path)\n",
    "print(f'Changed working directory to {project_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install YOLOv8\n",
    "Install the Ultralytics YOLOv8 package and other required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install YOLOv8\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install other required dependencies\n",
    "!pip install opencv-python-headless\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "\n",
    "# Verify the installation\n",
    "import ultralytics\n",
    "print(f'Ultralytics YOLOv8 version: {ultralytics.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Clothing Dataset\n",
    "Download and prepare a clothing dataset (like DeepFashion or Fashion-MNIST), organize images into appropriate folders for classification and segmentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the iMaterialist Fashion 2019 dataset for classification and segmentation\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Install Kaggle API for dataset download\n",
    "!pip install kaggle\n",
    "\n",
    "# Create directory for Kaggle API key\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# Upload your Kaggle API key (kaggle.json)\n",
    "print(\"Please upload your kaggle.json file (get it from https://www.kaggle.com/me/account)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move API key to the kaggle directory\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Create project directories\n",
    "project_path = \"/content/drive/MyDrive/clothing_project\" if os.path.exists(\"/content/drive\") else \"/content/clothing_project\"\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = os.path.join(project_path, 'iMaterialist_dataset')\n",
    "classification_dir = os.path.join(project_path, 'classification')\n",
    "segmentation_dir = os.path.join(project_path, 'segmentation')\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [dataset_path, classification_dir, segmentation_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Download the dataset (both train and validation sets)\n",
    "print(\"Downloading iMaterialist Fashion 2019 dataset...\")\n",
    "!kaggle competitions download -c imaterialist-fashion-2019-FGVC6 -p {dataset_path}\n",
    "\n",
    "# Unzip the downloaded files\n",
    "print(\"Extracting dataset files...\")\n",
    "!unzip -q {dataset_path}/imaterialist-fashion-2019-FGVC6.zip -d {dataset_path}\n",
    "\n",
    "# Load annotations\n",
    "train_annotations = json.load(open(os.path.join(dataset_path, 'train.json')))\n",
    "val_annotations = json.load(open(os.path.join(dataset_path, 'validation.json')))\n",
    "\n",
    "# Create directories for YOLO format\n",
    "train_images_dir = os.path.join(classification_dir, 'train/images')\n",
    "val_images_dir = os.path.join(classification_dir, 'valid/images')\n",
    "train_masks_dir = os.path.join(segmentation_dir, 'train/images')\n",
    "val_masks_dir = os.path.join(segmentation_dir, 'valid/images')\n",
    "train_labels_dir = os.path.join(segmentation_dir, 'train/labels')\n",
    "val_labels_dir = os.path.join(segmentation_dir, 'valid/labels')\n",
    "\n",
    "# Create all directories\n",
    "for directory in [train_images_dir, val_images_dir, train_masks_dir, val_masks_dir, train_labels_dir, val_labels_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "for image in tqdm(train_annotations['images'][:5000]):  # Limiting to 5000 images to save space\n",
    "    img_id = image['id']\n",
    "    img_file = f\"{img_id}.jpg\"\n",
    "    \n",
    "    # Copy image for classification\n",
    "    if os.path.exists(os.path.join(dataset_path, 'train', img_file)):\n",
    "        shutil.copy(\n",
    "            os.path.join(dataset_path, 'train', img_file),\n",
    "            os.path.join(train_images_dir, img_file)\n",
    "        )\n",
    "        # Also copy for segmentation (same images used for both tasks)\n",
    "        shutil.copy(\n",
    "            os.path.join(dataset_path, 'train', img_file),\n",
    "            os.path.join(train_masks_dir, img_file)\n",
    "        )\n",
    "\n",
    "# Process validation data\n",
    "print(\"Processing validation data...\")\n",
    "for image in tqdm(val_annotations['images']):\n",
    "    img_id = image['id']\n",
    "    img_file = f\"{img_id}.jpg\"\n",
    "    \n",
    "    # Copy image for classification\n",
    "    if os.path.exists(os.path.join(dataset_path, 'validation', img_file)):\n",
    "        shutil.copy(\n",
    "            os.path.join(dataset_path, 'validation', img_file),\n",
    "            os.path.join(val_images_dir, img_file)\n",
    "        )\n",
    "        # Also copy for segmentation\n",
    "        shutil.copy(\n",
    "            os.path.join(dataset_path, 'validation', img_file),\n",
    "            os.path.join(val_masks_dir, img_file)\n",
    "        )\n",
    "\n",
    "# Create a categories lookup dictionary\n",
    "categories = {cat['id']: cat['name'] for cat in train_annotations['categories']}\n",
    "\n",
    "# Save categories to a CSV file for reference\n",
    "pd.DataFrame(list(categories.items()), columns=['id', 'category']).to_csv(\n",
    "    os.path.join(project_path, 'categories.csv'), index=False\n",
    ")\n",
    "\n",
    "# Convert annotations to YOLO format for segmentation\n",
    "# This converts the segmentation masks to YOLO format\n",
    "print(\"Converting annotations to YOLO format...\")\n",
    "# This part would require processing the annotations to create YOLO-compatible label files\n",
    "# (Simplified for space constraints - would need to process segmentations from annotations)\n",
    "\n",
    "print(f\"Dataset preparation complete!\")\n",
    "print(f\"Images for classification: {len(os.listdir(train_images_dir))} training, {len(os.listdir(val_images_dir))} validation\")\n",
    "print(f\"Categories: {len(categories)}\")\n",
    "print(f\"Dataset directory: {project_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure YOLOv8 for Classification and Segmentation\n",
    "Set up YOLOv8 configurations for both classification and segmentation tasks, create YAML configuration files for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure YOLOv8 for Classification and Segmentation\n",
    "\n",
    "# Create YAML configuration files for the dataset\n",
    "classification_yaml = \"\"\"\n",
    "train: {classification_dir}/train\n",
    "val: {classification_dir}/val\n",
    "nc: 10  # Number of classes\n",
    "names: ['T-shirt', 'Shirt', 'Dress', 'Jeans', 'Shorts', 'Skirt', 'Jacket', 'Sweater', 'Shoes', 'Bag']\n",
    "\"\"\"\n",
    "\n",
    "segmentation_yaml = \"\"\"\n",
    "train: {segmentation_dir}/train\n",
    "val: {segmentation_dir}/val\n",
    "nc: 10  # Number of classes\n",
    "names: ['T-shirt', 'Shirt', 'Dress', 'Jeans', 'Shorts', 'Skirt', 'Jacket', 'Sweater', 'Shoes', 'Bag']\n",
    "\"\"\"\n",
    "\n",
    "# Save the YAML configuration files\n",
    "with open(os.path.join(project_path, 'classification.yaml'), 'w') as file:\n",
    "    file.write(classification_yaml)\n",
    "\n",
    "with open(os.path.join(project_path, 'segmentation.yaml'), 'w') as file:\n",
    "    file.write(segmentation_yaml)\n",
    "\n",
    "print('YAML configuration files created for classification and segmentation tasks.')\n",
    "\n",
    "# Load YOLOv8 model for classification\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize the model for classification\n",
    "model_classification = YOLO('yolov8n-cls.pt')  # Use a pre-trained YOLOv8 model for classification\n",
    "\n",
    "# Configure the model for training\n",
    "model_classification.train(data=os.path.join(project_path, 'classification.yaml'), epochs=50, imgsz=224)\n",
    "\n",
    "# Load YOLOv8 model for segmentation\n",
    "model_segmentation = YOLO('yolov8n-seg.pt')  # Use a pre-trained YOLOv8 model for segmentation\n",
    "\n",
    "# Configure the model for training\n",
    "model_segmentation.train(data=os.path.join(project_path, 'segmentation.yaml'), epochs=50, imgsz=640)\n",
    "\n",
    "print('YOLOv8 models configured for classification and segmentation tasks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Classification Model\n",
    "Train YOLOv8 classification model on the clothing dataset with appropriate hyperparameters and augmentation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classification Model\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU')\n",
    "\n",
    "# Initialize the YOLOv8 model for classification\n",
    "model_classification = YOLO('yolov8n-cls.pt')  # Use a pre-trained YOLOv8 model for classification\n",
    "\n",
    "# Configure the model for training\n",
    "model_classification.train(data=os.path.join(project_path, 'classification.yaml'), epochs=50, imgsz=224, device=device)\n",
    "\n",
    "print('YOLOv8 classification model training completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Segmentation Model\n",
    "Train YOLOv8 segmentation model to identify clothing items and create masks for background removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Segmentation Model\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU is not available, using CPU')\n",
    "\n",
    "# Initialize the YOLOv8 model for segmentation\n",
    "model_segmentation = YOLO('yolov8n-seg.pt')  # Use a pre-trained YOLOv8 model for segmentation\n",
    "\n",
    "# Configure the model for training\n",
    "model_segmentation.train(data=os.path.join(project_path, 'segmentation.yaml'), epochs=50, imgsz=640, device=device)\n",
    "\n",
    "print('YOLOv8 segmentation model training completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "Evaluate both models using metrics like precision, recall, mAP, and visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate classification model\n",
    "def evaluate_classification_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate segmentation model\n",
    "def evaluate_segmentation_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming data loaders for validation data are available as val_loader_classification and val_loader_segmentation\n",
    "# Evaluate the classification model\n",
    "evaluate_classification_model(model_classification, val_loader_classification)\n",
    "\n",
    "# Evaluate the segmentation model\n",
    "evaluate_segmentation_model(model_segmentation, val_loader_segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Background Removal\n",
    "Implement a pipeline to use the trained segmentation model to remove backgrounds from clothing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Background Removal\n",
    "\n",
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Function to remove background using the segmentation model\n",
    "def remove_background(image_path, model):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Preprocess the image for the model\n",
    "    input_image = cv2.resize(image_rgb, (640, 640))\n",
    "    input_image = input_image / 255.0\n",
    "    input_image = np.transpose(input_image, (2, 0, 1))\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    input_image = torch.tensor(input_image, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Get the segmentation mask from the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)[0]\n",
    "    mask = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Resize the mask to the original image size\n",
    "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Create a binary mask\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "    \n",
    "    # Apply the mask to the image\n",
    "    result = cv2.bitwise_and(image_rgb, image_rgb, mask=binary_mask)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "image_path = 'path_to_your_image.jpg'  # Replace with the path to your image\n",
    "result = remove_background(image_path, model_segmentation)\n",
    "\n",
    "# Display the original image and the result\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Background Removed')\n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export and Save the Models\n",
    "Export the trained models in appropriate formats (ONNX, TorchScript, etc.) and save them to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Save the Models\n",
    "\n",
    "# Export the trained classification model to ONNX format\n",
    "classification_onnx_path = os.path.join(project_path, 'yolov8_classification.onnx')\n",
    "model_classification.export(format='onnx', path=classification_onnx_path)\n",
    "print(f'Classification model exported to {classification_onnx_path}')\n",
    "\n",
    "# Export the trained segmentation model to ONNX format\n",
    "segmentation_onnx_path = os.path.join(project_path, 'yolov8_segmentation.onnx')\n",
    "model_segmentation.export(format='onnx', path=segmentation_onnx_path)\n",
    "print(f'Segmentation model exported to {segmentation_onnx_path}')\n",
    "\n",
    "# Save the trained classification model to TorchScript format\n",
    "classification_torchscript_path = os.path.join(project_path, 'yolov8_classification.pt')\n",
    "model_classification.save(path=classification_torchscript_path)\n",
    "print(f'Classification model saved to {classification_torchscript_path}')\n",
    "\n",
    "# Save the trained segmentation model to TorchScript format\n",
    "segmentation_torchscript_path = os.path.join(project_path, 'yolov8_segmentation.pt')\n",
    "model_segmentation.save(path=segmentation_torchscript_path)\n",
    "print(f'Segmentation model saved to {segmentation_torchscript_path}')\n",
    "\n",
    "# Save models to Google Drive\n",
    "drive_classification_onnx_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_classification.onnx'\n",
    "drive_segmentation_onnx_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_segmentation.onnx'\n",
    "drive_classification_torchscript_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_classification.pt'\n",
    "drive_segmentation_torchscript_path = '/content/drive/MyDrive/YOLOv8-Clothing-Classification/yolov8_segmentation.pt'\n",
    "\n",
    "shutil.copy(classification_onnx_path, drive_classification_onnx_path)\n",
    "shutil.copy(segmentation_onnx_path, drive_segmentation_onnx_path)\n",
    "shutil.copy(classification_torchscript_path, drive_classification_torchscript_path)\n",
    "shutil.copy(segmentation_torchscript_path, drive_segmentation_torchscript_path)\n",
    "\n",
    "print('Models saved to Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
